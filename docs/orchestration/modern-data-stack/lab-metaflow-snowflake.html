<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-orchestration/modern-data-stack/lab-metaflow-snowflake/README">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">Sequential Recommendation with the Modern Data Stack | Recohut Data Bootcamp</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://www.recohut.com/docs/orchestration/modern-data-stack/lab-metaflow-snowflake"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="data science, data engineering, data analytics"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Sequential Recommendation with the Modern Data Stack | Recohut Data Bootcamp"><meta data-rh="true" name="description" content="As a use case, we pick a popular RecSys challenge, session-based recommendation"><meta data-rh="true" property="og:description" content="As a use case, we pick a popular RecSys challenge, session-based recommendation"><link data-rh="true" rel="icon" href="/img/branding/favicon-black.svg"><link data-rh="true" rel="canonical" href="https://www.recohut.com/docs/orchestration/modern-data-stack/lab-metaflow-snowflake"><link data-rh="true" rel="alternate" href="https://www.recohut.com/docs/orchestration/modern-data-stack/lab-metaflow-snowflake" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.recohut.com/docs/orchestration/modern-data-stack/lab-metaflow-snowflake" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Recohut Data Bootcamp RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Recohut Data Bootcamp Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-B4S1B1ZDTT"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-B4S1B1ZDTT",{})</script><link rel="stylesheet" href="/assets/css/styles.099cbecb.css">
<link rel="preload" href="/assets/js/runtime~main.1db50233.js" as="script">
<link rel="preload" href="/assets/js/main.c578965a.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Bootcamp</b></a><a class="navbar__item navbar__link" href="/docs/introduction">Docs</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/sparsh-ai/recohut" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_Pkmr"><kbd class="searchHint_iIMx">ctrl</kbd><kbd class="searchHint_iIMx">K</kbd></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><main class="docMainContainer_gTbr docMainContainerEnhanced_Uz_u"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Sequential Recommendation with the Modern Data Stack</h1><p>As a use case, we pick a popular RecSys challenge, session-based recommendation: given the interactions between a shopper and some products in a browsing session, can we train a model to predic what the next interaction will be? The flow is powered by our open-source <a href="https://github.com/coveooss/SIGIR-ecom-data-challenge" target="_blank" rel="noopener noreferrer">Coveo Data Challenge dataset</a> - as model, we train a vanilla LSTM, a model just complex enough to make good use of cloud computing. At a quick glance, this is what we are building:</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/214904864-f96be41c-7e02-497c-845b-6ec9a36b6bc0.jpg" alt="The post-modern stack" class="img_ev3q"></p><p>As usual, we show a working, end-to-end, real-world flow: while you can run it locally with few thousands sessions to get the basics, we suggest you to use the <code>MAX_SESSIONS</code> variable to appreciate how well the stack scales - with no code changes - as millions of events are pushed to the warehouse.</p><p>For an in-depth explanation of the philosophy behind the approach, please check the companion <a href="https://towardsdatascience.com/the-post-modern-stack-993ec3b044c1" target="_blank" rel="noopener noreferrer">blog post</a>, and the previous episodes / repos in <a href="https://towardsdatascience.com/tagged/mlops-without-much-ops" target="_blank" rel="noopener noreferrer">the series</a>. </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="pre-requisites">Pre-requisites<a class="hash-link" href="#pre-requisites" title="Direct link to heading">​</a></h2><p>The code is a self-contained recommender project; however, since we leverage best-in-class tools, some preliminary setup is required. Please make sure the requirements are satisfied, depending on what you wish to run - roughly in order of ascending complexity:</p><p><em>The basics: Metaflow, Snowflake and dbt</em></p><p>A Snowflake account is needed to host the data, and a working Metaflow + dbt setup is needed to run the flow; we <em>strongly</em> suggest to run <code>Metaflow on AWS</code> (as it is the intended setup), but with some minor modifications you should be able to run the flow with a local store as well. </p><ul><li><em>Snowflake account</em>: <a href="https://signup.snowflake.com" target="_blank" rel="noopener noreferrer">sign-up for a free trial</a>.</li><li><em>AWS account</em>: <a href="https://aws.amazon.com/free/" target="_blank" rel="noopener noreferrer">sign-up for a free AWS account</a>.</li><li><em>Metaflow on AWS</em>: <a href="https://docs.metaflow.org/metaflow-on-aws" target="_blank" rel="noopener noreferrer">follow the setup guide</a>.</li><li><em>dbt core setup</em>: on top of installing the open source package (already included in the <code>requirements.txt</code>), you need to point dbt to your Snowflake instance with the proper <a href="https://docs.getdbt.com/dbt-cli/configure-your-profile" target="_blank" rel="noopener noreferrer">dbt_profile</a>, Make sure the SCHEMA there matches with what is specified in the <code>.env</code> file (<code>SF_SCHEMA</code>).</li></ul><p><em>Adding experiment tracking</em></p><ul><li><em>Comet ML</em>: <a href="https://www.comet.ml/signup" target="_blank" rel="noopener noreferrer">sign-up for free</a> and get an api key. If you don&#x27;t want experiment tracking, make sure to comment out the Comet specific parts in the <code>train_model</code> step.</li></ul><p><em>Adding PaaS deployment</em></p><ul><li><em>SageMaker setup</em>: To deploy the model as a PaaS solution using SageMaker, the <code>IAM_SAGEMAKER_ROLE</code> parameter in the flow needs to contain a suitable IAM ROLE to deploy an endpoint and access the s3 bucket where Metaflow is storing the model artifact; if you don&#x27;t wish to deploy your model, run the flow with <code>SAGEMAKER_DEPLOY=0</code> in the <code>.env</code> file.</li></ul><p><em>Adding dbt cloud</em></p><ul><li><em>dbt cloud account</em>: <a href="https://www.getdbt.com/signup" target="_blank" rel="noopener noreferrer">sign-up for free</a> and get an api key. If you don&#x27;t wish to use dbt cloud but just the local setup,set <code>DBT_CLOUD=0</code> in the <code>.env</code> file.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="virtual-env">Virtual env<a class="hash-link" href="#virtual-env" title="Direct link to heading">​</a></h3><p>Setup a virtual environment with the project dependencies:</p><ul><li><code>python -m venv venv</code></li><li><code>source venv/bin/activate</code></li><li><code>pip install -r requirements.txt</code></li></ul><p>NOTE: the current version of RecList has some old dependencies which may results in some (harmless) pip conflicts - conflicts will disappear with the new version, coming out soon.</p><p>Create a local version of the <code>local.env</code> file named only <code>.env</code> (do <em>not</em> commit it!), and make sure to fill its values properly:</p><table><thead><tr><th>VARIABLE</th><th>TYPE</th><th>MEANING</th></tr></thead><tbody><tr><td>SF_USER</td><td>string</td><td>Snowflake user name</td></tr><tr><td>SF_PWD</td><td>string</td><td>Snowflake password</td></tr><tr><td>SF_ACCOUNT</td><td>string</td><td>Snowflake account</td></tr><tr><td>SF_DB</td><td>string</td><td>Snowflake database</td></tr><tr><td>SF_SCHEMA</td><td>string (suggested: POST_MODERN_DATA_STACK)</td><td>Snowflake schema for raw and transformed data</td></tr><tr><td>SF_TABLE</td><td>string (COVEO_DATASET_RAW)</td><td>Snowflake table for raw data</td></tr><tr><td>SF_ROLE</td><td>string</td><td>Snowflake role to run SQL</td></tr><tr><td>APPLICATION_API_KEY</td><td>uuid (474d1224-e231-42ed-9fc9-058c2a8347a5)</td><td>Organization id to simulate a SaaS company</td></tr><tr><td>MAX_SESSIONS</td><td>int (1000)</td><td>Number of raw sessions to load into Snowflake (try first running the project locally with a small number)</td></tr><tr><td>EN_BATCH</td><td>0-1 (0)</td><td>Enable/disable cloud computing for @batch steps in Metaflow (try first running the project locally)</td></tr><tr><td>COMET_API_KEY</td><td>string</td><td>Comet ML api key</td></tr><tr><td>DBT_CLOUD</td><td>0-1 (0)</td><td>Enable/disable running dbt on the cloud</td></tr><tr><td>SAGEMAKER_DEPLOY</td><td>0-1 (1)</td><td>Enable/disable deploying the model artifact to a Sagemaker endpoint</td></tr><tr><td>DBT_ACCOUNT_ID</td><td>int</td><td>dbt cloud account id (you can find it in the dbt cloud URL)</td></tr><tr><td>DBT_PROJECT_ID</td><td>int</td><td>dbt cloud project id  (you can find it in the dbt cloud URL)</td></tr><tr><td>DBT_JOB_ID</td><td>int</td><td>dbt cloud job id (you can find it in the dbt cloud URL)</td></tr><tr><td>DBT_API_KEY</td><td>string</td><td>dbt cloud api key</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="load-data-into-snowflake">Load data into Snowflake<a class="hash-link" href="#load-data-into-snowflake" title="Direct link to heading">​</a></h3><p>Original datasets are from the Coveo SIGIR Data Challenge. To save you from downloading the original data dump and dealing with large text files, we re-used the abstraction over the data provided by RecList. If you run <code>upload_to_snowflake.py</code> in the <code>upload</code> folder from your laptop as a one-off script, the program will download the Data Challenge dataset and dump it to a Snowflake table that simulates the <a href="https://towardsdatascience.com/the-modern-data-pattern-d34d42216c81" target="_blank" rel="noopener noreferrer">append-only log pattern</a>. This allows us to use dbt and Metaflow to run a realistic ELT and ML code over real-world data.</p><p>Once you run the script, check your Snowflake for the new schema/table:</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/214904863-10c4a912-73e3-4ec9-b7c7-3c9c2de4d63d.png" alt="Raw table in Snowflake" class="img_ev3q"></p><p>If you wish to see how a data ingestion pipeline works (i.e. an endpoint streaming into Snowflake real-time, individual events, instead of a bulk upload), we open-sourced a <a href="https://github.com/jacopotagliabue/paas-data-ingestion" target="_blank" rel="noopener noreferrer">serverless pipeline</a> as well.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dbt">dbt<a class="hash-link" href="#dbt" title="Direct link to heading">​</a></h3><p>While we will run dbt code as part of Metaflow, it is good practice to try and see if everything works from a stand-alone setup first. To run and test the dbt transformations, just <code>cd</code> into the <code>dbt</code> folder and run <code>dbt run --vars &#x27;{SF_SCHEMA: POST_MODERN_DATA_STACK, SF_TABLE: COVEO_DATASET_RAW}&#x27;</code>, where the <a href="https://docs.getdbt.com/docs/building-a-dbt-project/building-models/using-variables" target="_blank" rel="noopener noreferrer">variables</a> reflect the content of your <code>.env</code> file  (you can also run <code>dbt test</code>, if you like).</p><p>Once you run dbt, check your Snowflake for the views:</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/214904836-1b0948bd-5c07-434e-a9e4-ed47da98f6fd.png" alt="Views in Snowflake" class="img_ev3q"></p><p>The <code>DBT_CLOUD</code> variable (see above) controls whether transformations are run from <em>within the flow folder</em>, or from a dbt cloud account, by using dbt API to trigger the transformation on the cloud platform. If you want to leverage dbt cloud, make sure to manually <a href="https://docs.getdbt.com/docs/dbt-cloud/cloud-quickstart#create-a-new-job" target="_blank" rel="noopener noreferrer">create a job</a> on the platform, and then configure the relevant variables in the <code>.env</code> file. In our tests, we used the exact same <code>.sql</code> and <code>.yml</code> files that you find in this repository:</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/214904853-8b030e6a-7eca-4a05-9bd3-6c051e595f71.png" class="img_ev3q"></p><p>Please note that instead of having a local dbt folder, you could have your dbt code in a Github repo and then either clone it using Github APIs at runtime, or import it in dbt cloud and use the platform to run the code base.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-run-aka-the-whole-enchilada">How to run (a.k.a. the whole enchilada)<a class="hash-link" href="#how-to-run-aka-the-whole-enchilada" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="run-the-flow">Run the flow<a class="hash-link" href="#run-the-flow" title="Direct link to heading">​</a></h3><p>Once the above setup steps are completed, you can run the flow:</p><ul><li>cd into the <code>src</code> folder;</li><li>run the flow with <code>METAFLOW_PROFILE=metaflow AWS_PROFILE=tooso AWS_DEFAULT_REGION=us-west-2 python my_dbt_flow.py --package-suffixes &quot;.py&quot; run --max-workers 4</code>, where <code>METAFLOW_PROFILE</code> is needed to select a specific Metaflow config (you can omit it, if you&#x27;re using the default), <code>AWS_PROFILE</code> is needed to select a specific AWS config that runs the flow and it&#x27;s related AWS infrastructure (you can omit it, if you&#x27;re using the default), and <code>AWS_DEFAULT_REGION</code> is needed to specify the target AWS region (you can omit it, if you&#x27;ve it already specified in your local AWS PROFILE and you do not wish to change it);</li><li>visualize the performance card with <code>METAFLOW_PROFILE=metaflow AWS_PROFILE=tooso AWS_DEFAULT_REGION=us-west-2 python my_dbt_flow.py card view test_model --id recCard</code> (see below for an intro to <a href="https://github.com/jacopotagliabue/reclist" target="_blank" rel="noopener noreferrer">RecList</a>).</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="results">Results<a class="hash-link" href="#results" title="Direct link to heading">​</a></h3><p>If you run the fully-featured flow (i.e. <code>SAGEMAKER_DEPLOY=1</code>) with the recommended setup, you will end up with:</p><ul><li>an up-to-date view in Snowflake, leveraging dbt to make raw data ready for machine learning; </li><li>versioned datasets and model artifacts in your AWS, accessible through the standard <a href="https://docs.metaflow.org/metaflow/client" target="_blank" rel="noopener noreferrer">Metaflow client API</a>;</li><li>a Comet dashboard for experiment tracking of the deep learning model, displaying training stats;</li><li>a versioned Metaflow card containing (some of) the tests run with RecList (see below);</li><li>finally, a DL-based, sequential recommender system serving predictions in real-time using SageMaker for inference.</li></ul><p>If you log in into your AWS SageMaker interface, you should find the new endpoint for next event prediction available for inference:</p><p> <img loading="lazy" src="https://user-images.githubusercontent.com/62965911/214904840-4fc64958-bf3c-476f-854b-e1fe82604d1b.png" alt="aws sagemaker UI" class="img_ev3q"></p><p>If you run the flow with dbt cloud, you will also find the dbt run in the history section on the cloud platform, easily identifiable through the flow id and user. </p><p> <img loading="lazy" src="https://user-images.githubusercontent.com/62965911/214904858-962297b8-4b31-420e-97c0-b53a0a9e03d0.png" alt="dbt run history" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="bonus-reclist-and-metaflow-cards">BONUS: RecList and Metaflow cards<a class="hash-link" href="#bonus-reclist-and-metaflow-cards" title="Direct link to heading">​</a></h3><p>The project includes a (stub of a) custom <a href="https://outerbounds.com/blog/integrating-pythonic-visual-reports-into-ml-pipelines/" target="_blank" rel="noopener noreferrer">DAG card</a> showing how the model is performing according to <a href="https://github.com/jacopotagliabue/reclist" target="_blank" rel="noopener noreferrer">RecList</a>, our open-source framework for behavioral testing. We could devote an <a href="https://towardsdatascience.com/ndcg-is-not-all-you-need-24eb6d2f1227" target="_blank" rel="noopener noreferrer">article</a> / <a href="https://arxiv.org/abs/2111.09963" target="_blank" rel="noopener noreferrer">paper</a> just to this (as we actually did recently!); you can visualize it with <code>METAFLOW_PROFILE=metaflow AWS_PROFILE=tooso AWS_DEFAULT_REGION=us-west-2 python my_dbt_flow.py card view test_model --id recCard</code> at the end of your run. No matter how small, we wanted to include the card/test as a reminder of <em>how important is to understand model behavior before deployment</em>. Cards are a natural UI to display some of the RecList information: since readable, shareable (self-)documentation is crucial for production, RecList new major release will include out-of-the-box support for visualization and reporting tools: reach out if you&#x27;re interested!</p><p>As a <em>bonus</em> bonus feature (thanks Valay for the snippet!), <em>only when running with the dbt core setup</em>, the (not-production-ready) function <code>get_dag_from_manifest</code> will read the local manifest file and produce a dictionary compatible with Metaflow Card API. If you type <code>METAFLOW_PROFILE=metaflow AWS_PROFILE=tooso AWS_DEFAULT_REGION=us-west-2 python my_dbt_flow.py card view run_transformation --id dbtCard</code> at the end of a successful run, you should see a card displaying the dbt card <em>as a Metaflow card</em>, as in the image below:</p><p> <img loading="lazy" src="https://user-images.githubusercontent.com/62965911/214904847-62dad743-5f3a-4bb9-8a9d-33e121747dd1.png" alt="dbt card on Metaflow" class="img_ev3q"></p><p> We leave to the reader (and / or to future iterations) to explore how to combine dbt, RecList and other info into a custom, well-designed card!</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="whats-next">What&#x27;s next?<a class="hash-link" href="#whats-next" title="Direct link to heading">​</a></h2><p>Of course, the post-modern stack can be further expanded or improved in many ways. Without presumption of completeness, these are some ideas to start:</p><ul><li>on the dataOps side, we could include some data quality checks, either by improving our dbt setup, or by introducing additional tooling: at <a href="https://towardsdatascience.com/hagakure-for-mlops-the-four-pillars-of-ml-at-reasonable-scale-5a09bd073da" target="_blank" rel="noopener noreferrer">reasonable scale</a> the greater marginal value is typically to be found in better data, as compared to better models;</li><li>on the MLOps side, we barely scratched the surface: one side, we kept the modeling simple and avoid any tuning, which is however very easy to do using Metaflow built-in parallelization abilities; on the other, you may decide to complicate the flow with other tools, improve on serving etc. (e.g. the proposal <a href="https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat" target="_blank" rel="noopener noreferrer">here</a>). Swapping in-and-out different tools with similar functionalities should be easy: in a previous work, we <a href="https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat/blob/main/local_flow/rec/src/utils.py" target="_blank" rel="noopener noreferrer">abstracted away experiment tracking</a> and allow users to pick <a href="https://neptune.ai/" target="_blank" rel="noopener noreferrer">Neptune</a> as an alternative SaaS platform. Similar considerations apply to this use case as well;</li><li>a proper RecList for this flow is yet to be developed, as the current proposal is nothing more than a stub showing how easy it is to run a devoted test suite when needed: you can augment the simple suite we prepared, improve the visualization on cards or both - since RecList roadmap is quickly progressing, we expect a deeper integration and a whole new set of functionalities to be announced soon. Stay tuned for our next iteration on this!</li></ul><p>Is this the <em>only</em> way to run dbt in Metaflow? Of course not - in particular, you could think of writing a small wrapper around a flow and a dbt-core project that creates individual Metaflow steps corresponding to individual dbt steps, pretty much like suggested <a href="https://www.astronomer.io/blog/airflow-dbt-1/" target="_blank" rel="noopener noreferrer">here</a> for another orchestrator. But this is surely a story for another repo / time ;-)</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"></div><div class="col lastUpdated_vwxv"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2023-04-09T13:34:30.000Z">Apr 9, 2023</time></b> by <b>sparsh</b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#pre-requisites" class="table-of-contents__link toc-highlight">Pre-requisites</a></li><li><a href="#setup" class="table-of-contents__link toc-highlight">Setup</a><ul><li><a href="#virtual-env" class="table-of-contents__link toc-highlight">Virtual env</a></li><li><a href="#load-data-into-snowflake" class="table-of-contents__link toc-highlight">Load data into Snowflake</a></li><li><a href="#dbt" class="table-of-contents__link toc-highlight">dbt</a></li></ul></li><li><a href="#how-to-run-aka-the-whole-enchilada" class="table-of-contents__link toc-highlight">How to run (a.k.a. the whole enchilada)</a><ul><li><a href="#run-the-flow" class="table-of-contents__link toc-highlight">Run the flow</a></li><li><a href="#results" class="table-of-contents__link toc-highlight">Results</a></li><li><a href="#bonus-reclist-and-metaflow-cards" class="table-of-contents__link toc-highlight">BONUS: RecList and Metaflow cards</a></li></ul></li><li><a href="#whats-next" class="table-of-contents__link toc-highlight">What&#39;s next?</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 Bootcamp. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.1db50233.js"></script>
<script src="/assets/js/main.c578965a.js"></script>
</body>
</html>