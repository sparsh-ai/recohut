"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[19055],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>m});var n=a(67294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var s=n.createContext({}),c=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},p=function(e){var t=c(e.components);return n.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,i=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),d=c(a),m=o,f=d["".concat(s,".").concat(m)]||d[m]||u[m]||i;return a?n.createElement(f,r(r({ref:t},p),{},{components:a})):n.createElement(f,r({ref:t},p))}));function m(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=a.length,r=new Array(i);r[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:o,r[1]=l;for(var c=2;c<i;c++)r[c]=a[c];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},58299:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>u,frontMatter:()=>i,metadata:()=>l,toc:()=>c});var n=a(87462),o=(a(67294),a(3905));const i={title:"Fake Voice Detection",authors:"sparsh",tags:["audio","deepfake"]},r=void 0,l={permalink:"/blog/2021/10/01/fake-voice-detection",source:"@site/blog/2021-10-01-fake-voice-detection.mdx",title:"Fake Voice Detection",description:"/img/content-blog-raw-blog-fake-voice-detection-untitled.png",date:"2021-10-01T00:00:00.000Z",formattedDate:"October 1, 2021",tags:[{label:"audio",permalink:"/blog/tags/audio"},{label:"deepfake",permalink:"/blog/tags/deepfake"}],readingTime:2.88,hasTruncateMarker:!1,authors:[{name:"Sparsh Agarwal",title:"Data Scientist & Engineer",url:"https://github.com/sparsh-ai",email:"sprsag@gmail.com",imageURL:"https://github.com/sparsh-ai.png",key:"sparsh"}],frontMatter:{title:"Fake Voice Detection",authors:"sparsh",tags:["audio","deepfake"]},prevItem:{title:"Document Recommendation",permalink:"/blog/2021/10/01/document-recommendation"},nextItem:{title:"Image Similarity System",permalink:"/blog/2021/10/01/image-similarity-system"}},s={authorsImageUrls:[void 0]},c=[],p={toc:c};function u(e){let{components:t,...i}=e;return(0,o.kt)("wrapper",(0,n.Z)({},p,i,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"/img/content-blog-raw-blog-fake-voice-detection-untitled.png",src:a(24763).Z,width:"1920",height:"1080"})),(0,o.kt)("h1",{id:"introduction"},"Introduction"),(0,o.kt)("p",null,"Fake audio can be used for malicious purposes which affect directly or indirectly human life. The objective is to differentiate between fake and real voice. Python and deep learning has been used and implemented to achieve the objective. Audio files or video file are being used as an input of this work then model has been trained for uniquely identify features for voice creation and voice detection. Deep learning technique is used to find accuracy between real and fake."),(0,o.kt)("p",null,"Speaker recognition usually refers to both speaker identification and speaker verification. A speaker identification system identifies who the speaker is, while an automatic speaker verification (ASV) system decides if an identity claim is true or false.\nA general ASV system is robust to zero-effort impostors, they are\xa0vulnerable to more sophisticated attacks. Such vulnerability represents one of the security concerns of ASV systems.\xa0Spoofing involves an adversary (attacker) who masquerades as the target speaker to gain the access to a system.\xa0Such spoofing attacks can happen to various biometric traits, such as fingerprints, iris, face, and voice patterns. We are focusing only on the voice-based spoofing and anti-spoofing techniques for ASV system.\xa0The spoofed speech samples can be obtained through speech synthesis, voice conversion, or replay of recorded speech.\xa0",(0,o.kt)("strong",{parentName:"p"},"Imagine the following scenario\u2026"),"\nYour phone rings, you pick up. It\u2019s your spouse asking you for details about your savings account \u2014 they don\u2019t have the account information on hand, but want to deposit money there this afternoon. Later, you realize a bunch of money has went missing! After investigating, you find out that the person masquerading as them on the other line was a voice 100% generated with AI. You\u2019ve just been scammed, and on top of that, can\u2019t believe the voice you thought belonged to your spouse was actually a fake."),(0,o.kt)("p",null,"To discern between real and fake audio, the detector uses visual representations of audio clips called spectrograms, which are also used to train speech synthesis models.\nGoogle\u2019s 2019\xa0",(0,o.kt)("a",{parentName:"p",href:"https://www.blog.google/outreach-initiatives/google-news-initiative/advancing-research-fake-audio-detection/"},"AVSSpoof dataset"),"\xa0contains over 25,000 clips of audio, featuring both real and fake clips of a variety of male and female speakers.",(0,o.kt)("strong",{parentName:"p"},"Temporal Convolution Model")),(0,o.kt)("h1",{id:"modeling-approach"},"Modeling Approach"),(0,o.kt)("p",null,"First, raw audio is preprocessed and converted into a mel-frequency spectrogram \u2014 this is the input for the model. The model performs convolutions over the time dimension of the spectrogram, then uses masked pooling to prevent overfitting. Finally, the output is passed into a dense layer and a sigmoid activation function, which ultimately outputs a predicted probability between 0 (fake) and 1 (real).\nThe baseline model achieved 99%, 95%, and 85% accuracy on the train, validation, and test sets respectively. The differing performance is caused by differences between the three datasets. While all three datasets feature distinct and different speakers, the test set uses a different set of fake audio generating algorithms that were not present in the train or validation set."),(0,o.kt)("h1",{id:"proposed-framework"},"Proposed Framework"),(0,o.kt)("h1",{id:"process-flow"},"Process Flow"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Voice detection",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Temporal Convolution model",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Install packages"),(0,o.kt)("li",{parentName:"ul"},"Download pretrained models"),(0,o.kt)("li",{parentName:"ul"},"Initialize the model"),(0,o.kt)("li",{parentName:"ul"},"Load data"),(0,o.kt)("li",{parentName:"ul"},"Detect DeepFakes"))),(0,o.kt)("li",{parentName:"ul"},"GMM-UBG model",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Install packages"),(0,o.kt)("li",{parentName:"ul"},"Train the model"),(0,o.kt)("li",{parentName:"ul"},"Load data"),(0,o.kt)("li",{parentName:"ul"},"Detect DeepFakes"))),(0,o.kt)("li",{parentName:"ul"},"Convolutional VAE model",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Install packages"),(0,o.kt)("li",{parentName:"ul"},"Train the model"),(0,o.kt)("li",{parentName:"ul"},"Load data"),(0,o.kt)("li",{parentName:"ul"},"Detect DeepFakes"))),(0,o.kt)("li",{parentName:"ul"},"Voice Similarity",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Install packages"),(0,o.kt)("li",{parentName:"ul"},"Load data"),(0,o.kt)("li",{parentName:"ul"},"Voice similarity match"),(0,o.kt)("li",{parentName:"ul"},"Embedding visualization")))))),(0,o.kt)("h1",{id:"models-algorithms"},"Models Algorithms"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Temporal Convolution"),(0,o.kt)("li",{parentName:"ol"},"ResNet"),(0,o.kt)("li",{parentName:"ol"},"GMM"),(0,o.kt)("li",{parentName:"ol"},"Light CNN"),(0,o.kt)("li",{parentName:"ol"},"Fusion"),(0,o.kt)("li",{parentName:"ol"},"SincNet"),(0,o.kt)("li",{parentName:"ol"},"ASSERT"),(0,o.kt)("li",{parentName:"ol"},"HOSA"),(0,o.kt)("li",{parentName:"ol"},"CVAE")))}u.isMDXComponent=!0},24763:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/content-blog-raw-blog-fake-voice-detection-untitled-f11b5243c4377e8265e1a521da103c79.png"}}]);