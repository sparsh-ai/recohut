{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea6064e3-ca9a-46e2-a617-ea5c8a0c0704",
   "metadata": {},
   "source": [
    "# Current Approach to Identifying Tweets needing Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44fe1f40-64d2-4b10-88d5-d4c28f5bc8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black==22.6.0\n",
      "boto3==1.24.56\n",
      "dask==2022.8.0\n",
      "dask-ml==2022.5.27\n",
      "distributed==2022.8.0\n",
      "nb-black==1.0.7\n",
      "pandas==1.4.3\n",
      "s3fs==0.4.2\n",
      "scikit-learn==1.1.2\n",
      "ipykernel                 6.15.1             pyh210e3f2_0    conda-forge\n",
      "CPU times: user 40.6 ms, sys: 13.4 ms, total: 54 ms\n",
      "Wall time: 2.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip3 freeze | grep -E 'boto3|s3fs|scikit-learn|distributed|dask==|dask-m|black==|jupyter-server|pandas'\n",
    "!conda list -n spark | grep -E 'ipykernel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5966485-5e45-4d55-be8c-5276f0b64ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa4af8e2-df5f-41cd-adca-c0cede5a2b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "\n",
    "import boto3\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as skm\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split as sk_train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dfe1a3-021d-4e19-823a-92f88fb62bab",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5dbeeb-b085-48d8-a573-7419d42e086a",
   "metadata": {},
   "source": [
    "This notebook walks through the end-to-end workflow being currently used to identify negative sentiment tweets that need to be reviewed by mission team members.\n",
    "\n",
    "Machine learning scoring metrics identified in `scoping.md` are used to quantitatively evaluate this approach. The business metric is time wasted reading non-negative sentiment tweets and this is also estimated here.\n",
    "\n",
    "For a summary of assumptions made, see the discussion at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68c1212-c1d5-4c4a-afc0-05c147096a30",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18cde7a8-036c-4074-bdfa-f2e70abeac54",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "label_mapper = {0: \"does_not_need_support\", 1: \"needs_support\"}\n",
    "\n",
    "nrows = 172_000\n",
    "partition_size = 21_000\n",
    "frac_negative = 0.045  # for dummy data only\n",
    "\n",
    "test_split_frac = 0.125\n",
    "\n",
    "# inference\n",
    "inference_start_date = \"2022-01-10 00:00:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ca9c522-d115-4af0-8e18-9fec671e0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_partitions = int(nrows / partition_size)\n",
    "\n",
    "val_split_frac = test_split_frac / (1 - test_split_frac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ac42b1-4750-449f-85be-307bb2a3bf70",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a46c0f8-546c-4978-bbc0-67e1b07ab465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label text\n",
       "0      0    A\n",
       "1      0    A\n",
       "2      0    A\n",
       "3      0    A\n",
       "4      0    A"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.6 ms, sys: 6.74 ms, total: 21.3 ms\n",
      "Wall time: 22 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(np.random.randint(0, 1, nrows-int(frac_negative*nrows)), columns=['label']),\n",
    "        pd.DataFrame(np.random.randint(1, 2, int(frac_negative*nrows)), columns=['label']),\n",
    "    ], ignore_index=True\n",
    ").assign(text='A')\n",
    "ddf = dd.from_pandas(df, npartitions=n_partitions)\n",
    "display(ddf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0766fc8-9cc4-444f-8f0b-44edb365779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.53 ms, sys: 205 Âµs, total: 1.74 ms\n",
      "Wall time: 1.67 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = ddf[['text']]\n",
    "y = ddf['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e238d753-1c32-45f8-a116-57c79bf30517",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5306d872-add2-4bcc-bb4c-996188d34277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 ms, sys: 0 ns, total: 9 ms\n",
      "Wall time: 9.22 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=test_split_frac, random_state=88, shuffle=True\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=val_split_frac, random_state=88, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd62183-4a58-4377-a34f-3e6a4ed08ef8",
   "metadata": {},
   "source": [
    "Show the class distribution of the label (*needs support* vs *does not need support*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05151d93-fef9-4146-8738-0bfef5704e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 70.3 ms, sys: 17.1 ms, total: 87.4 ms\n",
      "Wall time: 87.5 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_tweets</th>\n",
       "      <th>frac_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123292</td>\n",
       "      <td>0.954842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5831</td>\n",
       "      <td>0.045158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_tweets  frac_tweets\n",
       "0      123292     0.954842\n",
       "1        5831     0.045158"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "class_distribution_train = (\n",
    "    y_train.value_counts()\n",
    "    .rename(\"num_tweets\")\n",
    "    .compute()\n",
    "    .to_frame()\n",
    ")\n",
    "class_distribution_train = class_distribution_train.assign(\n",
    "    frac_tweets=lambda df: df[\"num_tweets\"] / df[\"num_tweets\"].sum(axis=0)\n",
    ")\n",
    "class_distribution_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f94269-cd1d-4ba2-813c-04ba12656391",
   "metadata": {},
   "source": [
    "## (Naive) Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b067e3a1-60d9-4d3c-bed9-72ab1d82ef2c",
   "metadata": {},
   "source": [
    "Define the ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6ded0de-88be-40c8-ba4f-bf18165cabef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"clf\", DummyClassifier(strategy=\"uniform\", random_state=88))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60818707-3bc1-499f-9f54-f66ac922fea0",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5036d4ba-3833-4b34-bdae-6516d77e713b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 105 ms, sys: 18.2 ms, total: 123 ms\n",
      "Wall time: 193 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = pipe.fit(X_train_val, y_train_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c8e35-ba0f-4146-b099-b28182c9af8f",
   "metadata": {},
   "source": [
    "Make predictions on the tweets in the test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6289c82-7b90-4661-8f87-7a536eb742a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 75.3 ms, sys: 25.6 ms, total: 101 ms\n",
      "Wall time: 80.8 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6935</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11244</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17922</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "3725       0\n",
       "6935       0\n",
       "11244      1\n",
       "728        1\n",
       "17922      0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "y_test_pred = pd.Series(pipe.predict(X_test), name='label', index=y_test.index.compute())\n",
    "# ddf_test_pred = dd.from_pandas(y_test_pred.to_frame(), npartitions=y_test.npartitions)\n",
    "y_test_pred.head().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f228da-0012-4c7c-a9b3-eae80f19ae83",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "1. The predictions will be brought into memory since the pipeline is an in-memory object (`sklearn.pipeline.Pipeline`). However, since these are the test split predictions, we will **assume that the length of labels in the the test split is small enough to fit into local memory**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74409f6b-5ace-4268-9585-bd306a6c063c",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e43f66-66e6-45b3-8042-f0361b804c5d",
   "metadata": {},
   "source": [
    "Bring the test-split into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09910e4f-a820-4349-988b-9695760d4722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37 ms, sys: 6.39 ms, total: 43.4 ms\n",
      "Wall time: 51.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_test_computed = y_test.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b60cc-2b79-4f11-ba69-04646a0a74ce",
   "metadata": {},
   "source": [
    "### Scoring Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa32455-ddf5-4195-906d-e542ffef9f1d",
   "metadata": {},
   "source": [
    "Calculate evaluation metrics on the test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d444aa5d-144f-4494-8b7d-7b2628379680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>f2_score</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.497134</td>\n",
       "      <td>0.913809</td>\n",
       "      <td>0.497134</td>\n",
       "      <td>0.627629</td>\n",
       "      <td>0.532078</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision    recall  f1_score  f2_score split\n",
       "0  0.497134   0.913809  0.497134  0.627629  0.532078  test"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>does_not_need_support</th>\n",
       "      <td>0.955040</td>\n",
       "      <td>0.496628</td>\n",
       "      <td>0.653455</td>\n",
       "      <td>20317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>needs_support</th>\n",
       "      <td>0.045722</td>\n",
       "      <td>0.507772</td>\n",
       "      <td>0.083890</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       precision    recall  f1-score  support\n",
       "does_not_need_support   0.955040  0.496628  0.653455    20317\n",
       "needs_support           0.045722  0.507772  0.083890      965"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>does_not_need_support</th>\n",
       "      <th>needs_support</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>does_not_need_support</th>\n",
       "      <td>10090</td>\n",
       "      <td>10227</td>\n",
       "      <td>20317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>needs_support</th>\n",
       "      <td>475</td>\n",
       "      <td>490</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>10565</td>\n",
       "      <td>10717</td>\n",
       "      <td>21282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       does_not_need_support  needs_support  total\n",
       "does_not_need_support                  10090          10227  20317\n",
       "needs_support                            475            490    965\n",
       "total                                  10565          10717  21282"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 125 ms, sys: 0 ns, total: 125 ms\n",
      "Wall time: 131 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "metrics_dict = dict(\n",
    "    accuracy=skm.accuracy_score(y_test_computed, y_test_pred),\n",
    "    precision=skm.precision_score(y_test_computed, y_test_pred, average='weighted'),\n",
    "    recall=skm.recall_score(y_test_computed, y_test_pred, average='weighted'),\n",
    "    f1_score=skm.f1_score(y_test_computed, y_test_pred, average='weighted'),\n",
    "    f2_score=skm.fbeta_score(y_test_computed, y_test_pred, beta=2, average='weighted'),\n",
    ")\n",
    "df_cr = pd.DataFrame(\n",
    "    skm.classification_report(\n",
    "        y_test_computed,\n",
    "        y_test_pred,\n",
    "        labels=list(label_mapper),\n",
    "        target_names=list(label_mapper.values()),\n",
    "        output_dict=True,\n",
    "    )\n",
    ").T.iloc[:2].astype({\"support\": pd.Int32Dtype()})\n",
    "df_cm = pd.DataFrame(\n",
    "    skm.confusion_matrix(y_test_computed, y_test_pred, labels=list(label_mapper))\n",
    ").rename(columns=label_mapper)\n",
    "df_cm.index = df_cm.index.map(label_mapper)\n",
    "df_cm = pd.concat(\n",
    "    [\n",
    "        df_cm.assign(total=lambda df: df.sum(axis=1)),\n",
    "        df_cm.sum(axis=0).rename(\"total\").to_frame().T,\n",
    "    ]\n",
    ").fillna(len(y_test)).astype({\"total\": pd.Int32Dtype()})\n",
    "df_metrics = pd.DataFrame.from_dict(metrics_dict, orient='index').T.assign(split='test')\n",
    "display(df_metrics)\n",
    "display(df_cr)\n",
    "display(df_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f3520-9667-4974-a6a8-a621ca1009f9",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. We had identified F1-score and F2-score as being the candidate scoring metrics for this use-case. The closer both values are to 1.0 the better the current ML model. The metrics are approximately 0.6 and 0.5 respectively, indicating that (strictly from the perspective of an ML metric) improvement is warranted.\n",
    "2. The classification report (second output above) shows F1-score dropping by nearly an order of magnitude for the minority class (negative or neutral sentiment tweets) compared to the majority class (positive sentiment tweets).\n",
    "2. Ultimately, we identified F2-score as being the primary metric to be used. With the current approach, this metric score is poor (approximately 0.53)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa277e9-2b50-4ed9-877a-be2723fdc5f1",
   "metadata": {},
   "source": [
    "### Business Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d6ce65-9dda-42da-af5f-d1678a5a075c",
   "metadata": {},
   "source": [
    "Next, we will calculate a business metric - the amount of time spent unnecessariliy reading tweets. This will refer to tweets with a positive sentiment that were being read by the mission's social media support team."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c84a581-fd03-46c8-b976-32bb313236f3",
   "metadata": {},
   "source": [
    "Use the third output above (confusion matrix) to summarize the number of unnecessarily read tweets below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3b50b75-c9ce-493a-8b08-ea1cc9d50f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number tweets unnecessarily read</th>\n",
       "      <th>total number tweets</th>\n",
       "      <th>fraction tweets unnecessarily read</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9752</td>\n",
       "      <td>21282</td>\n",
       "      <td>0.458228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number tweets unnecessarily read  total number tweets  \\\n",
       "0                              9752                21282   \n",
       "\n",
       "   fraction tweets unnecessarily read  \n",
       "0                            0.458228  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tweets_unnecessarily_read = (\n",
    "    df_cm.loc[\"total\", \"needs_support\"] - df_cm.loc[\"needs_support\", \"total\"]\n",
    ")\n",
    "\n",
    "df_reading_time_summary = (\n",
    "    pd.Series(\n",
    "        [\n",
    "            num_tweets_unnecessarily_read,\n",
    "            len(y_test),\n",
    "            num_tweets_unnecessarily_read / len(y_test),\n",
    "        ],\n",
    "        index=[\n",
    "            \"number tweets unnecessarily read\",\n",
    "            \"total number tweets\",\n",
    "            \"fraction tweets unnecessarily read\",\n",
    "        ],\n",
    "    )\n",
    "    .to_frame()\n",
    "    .T.astype(\n",
    "        {\n",
    "            \"number tweets unnecessarily read\": pd.Int32Dtype(),\n",
    "            \"total number tweets\": pd.Int32Dtype(),\n",
    "        }\n",
    "    )\n",
    ")\n",
    "df_reading_time_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25603a33-5175-4cab-b63d-cab0ffa285e4",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. Out of the 21,282 tweets on which the currently used (naive) ML model is being evaluated, the model predicted that\n",
    "   - 10,717 tweets need support\n",
    "   - 10,565 tweets do not need support\n",
    "\n",
    "   In reality\n",
    "   - 965 tweets need support\n",
    "   - 20,317 tweets do not need support\n",
    "\n",
    "   This means 9,752 (10,717 - 965, or approx. 45%) of the available tweets would unnecessarily be read (and responded to) by the mission team members who are acting in a support capacity, in order to mitigate negative sentiment on Twitter. If we **assume an average combined reading and responding time of one minute per tweet**, then this would amount to (9,752 tweets X 0.50 sec/tweet X 1 min/60 sec) an average of 81 hours of time wasted reading tweets that did not express a negative sentiment about the mission on the platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f686c-efa5-43dd-8600-a9514df84ed6",
   "metadata": {},
   "source": [
    "## Summary of Assumptions\n",
    "1. The length of the labels in the validation and test splits is small enough to fit into local memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a3be9-921e-4250-8321-aba851413cd2",
   "metadata": {},
   "source": [
    "<span style=\"float:left;\">\n",
    "    <a href=\"./6_nlp_labeling.ipynb\"><< 6 - NLP-based Labeling</a>\n",
    "</span>\n",
    "\n",
    "<span style=\"float:right;\">\n",
    "    <a href=\"./8_analysis.ipynb\">8 - Analysis >></a>\n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark:Python",
   "language": "python",
   "name": "conda-env-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
