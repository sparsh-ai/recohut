{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: PCollection[[4]: Create initial values/Map(decode).None]\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "inputs = [0, 1, 2, 3]\n",
    "\n",
    "# Create a pipeline.\n",
    "with beam.Pipeline() as pipeline:\n",
    "  # Feed it some input elements with `Create`.\n",
    "  outputs = (\n",
    "      pipeline\n",
    "      | 'Create initial values' >> beam.Create(inputs)\n",
    "  )\n",
    "\n",
    "  # `outputs` is a PCollection with our input elements.\n",
    "  # But printing it directly won't show us its contents :(\n",
    "  print(f\"outputs: {outputs}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note:` In Beam, you can NOT access the elements from a PCollection directly like a Python list. This means, we can't simply print the output PCollection to see the elements.\n",
    "\n",
    "This is because, depending on the runner, the PCollection elements might live in multiple worker machines.\n",
    "\n",
    "To print the elements in the PCollection, we'll do a little trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "inputs = [0, 1, 2, 3]\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "  outputs = (\n",
    "      pipeline\n",
    "      | 'Create initial values' >> beam.Create(inputs)\n",
    "  )\n",
    "\n",
    "  # We can only access the elements through another transform.\n",
    "  outputs | beam.Map(print)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Beam, there is the Map transform, but we must use it within a pipeline.\n",
    "\n",
    "First we create a pipeline and feed it our input elements. Then we pipe those elements into a Map transform where we apply our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "inputs = [0, 1, 2, 3]\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "  outputs = (\n",
    "      pipeline\n",
    "      | 'Create values' >> beam.Create(inputs)\n",
    "      | 'Multiply by 2' >> beam.Map(lambda x: x * 2)\n",
    "  )\n",
    "\n",
    "  outputs | beam.Map(print)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FlatMap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FlatMap accepts a function that takes a single input element and outputs an iterable of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "inputs = [0, 1, 2, 3]\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "  outputs = (\n",
    "      pipeline\n",
    "      | 'Create values' >> beam.Create(inputs)\n",
    "      | 'Expand elements' >> beam.FlatMap(lambda x: [x for _ in range(x)])\n",
    "  )\n",
    "\n",
    "  outputs | beam.Map(print)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter\n",
    "\n",
    "Sometimes we want to only process certain elements while ignoring others.\n",
    "\n",
    "We want to filter each element in a collection using a function.\n",
    "\n",
    "filter takes a function that checks a single element a, and returns True to keep the element, or False to discard it.\n",
    "\n",
    "â„¹ï¸ For example, we only want to keep number that are even, or divisible by two. We can use the modulo operator % for a simple check.\n",
    "\n",
    "In Beam, there is the Filter transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "inputs = [0, 1, 2, 3]\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "  outputs = (\n",
    "      pipeline\n",
    "      | 'Create values' >> beam.Create(inputs)\n",
    "      | 'Keep only even numbers' >> beam.Filter(lambda x: x % 2 == 0)\n",
    "  )\n",
    "\n",
    "  outputs | beam.Map(print)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine\n",
    "\n",
    "We also need a way to get a single value from an entire PCollection. We might want to get the total number of elements, or the average value, or any other type of aggregation of values.\n",
    "\n",
    "We want to combine the elements in a collection into a single output.\n",
    "\n",
    "combine takes a function that transforms an iterable of inputs a, and returns a single output a.\n",
    "\n",
    "Other common names for this function are fold and reduce.\n",
    "\n",
    "â„¹ï¸ For example, we want to add all numbers together.\n",
    "\n",
    "In Beam, there are aggregation transforms.\n",
    "\n",
    "For this particular example, we can use the CombineGlobally transform which accepts a function that takes an iterable of elements as an input and outputs a single value.\n",
    "\n",
    "We can pass the built-in function sum into CombineGlobally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "inputs = [0, 1, 2, 3]\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "  outputs = (\n",
    "      pipeline\n",
    "      | 'Create values' >> beam.Create(inputs)\n",
    "      | 'Sum all values together' >> beam.CombineGlobally(sum)\n",
    "  )\n",
    "\n",
    "  outputs | beam.Map(print)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â„¹ï¸ There are many ways to combine values in Beam. You could even combine them into a different data type by defining a customÂ `CombineFn`.\n",
    "\n",
    "You can learn more about them by checking the availableÂ [aggregation transforms](https://beam.apache.org/documentation/transforms/python/overview/#aggregation)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupByKey\n",
    "\n",
    "Sometimes it's useful to pair each element with a key that we can use to group related elements together.\n",
    "\n",
    "Think of it as creating a Python dict from a list of (key, value) pairs, but instead of replacing the value on a \"duplicate\" key, you would get a list of all the values associated with that key.\n",
    "\n",
    "â„¹ï¸ For example, we want to group each animal with the list of foods they like, and we start with (animal, food) pairs.\n",
    "\n",
    "In Beam, there is the GroupByKey transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ðŸ¹', ['ðŸŒ½', 'ðŸŒ°'])\n",
      "('ðŸ¼', ['ðŸŽ‹'])\n",
      "('ðŸ°', ['ðŸ¥•', 'ðŸ¥’'])\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "inputs = [\n",
    "  ('ðŸ¹', 'ðŸŒ½'),\n",
    "  ('ðŸ¼', 'ðŸŽ‹'),\n",
    "  ('ðŸ°', 'ðŸ¥•'),\n",
    "  ('ðŸ¹', 'ðŸŒ°'),\n",
    "  ('ðŸ°', 'ðŸ¥’'),\n",
    "]\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "  outputs = (\n",
    "      pipeline\n",
    "      | 'Create (animal, food) pairs' >> beam.Create(inputs)\n",
    "      | 'Group foods by animals' >> beam.GroupByKey()\n",
    "  )\n",
    "\n",
    "  outputs | beam.Map(print)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and writing data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've learned some of the basic transforms likeÂ [`Map`](https://beam.apache.org/documentation/transforms/python/elementwise/map),Â [`FlatMap`](https://beam.apache.org/documentation/transforms/python/elementwise/flatmap),Â [`Filter`](https://beam.apache.org/documentation/transforms/python/elementwise/filter),Â [`Combine`](https://beam.apache.org/documentation/transforms/python/aggregation/combineglobally), andÂ [`GroupByKey`](https://beam.apache.org/documentation/transforms/python/aggregation/groupbykey). These allow us to transform data in any way, but so far we've usedÂ [`Create`](https://beam.apache.org/documentation/transforms/python/other/create)Â to get data from an in-memoryÂ [`iterable`](https://docs.python.org/3/glossary.html#term-iterable), like aÂ `list`.\n",
    "\n",
    "This works well for experimenting with small datasets. For larger datasets we can useÂ `Source`Â transforms to read data andÂ `Sink`Â transforms to write data. If there are no built-inÂ `Source`Â orÂ `Sink`Â transforms, we can also easily create our custom I/O transforms.\n",
    "\n",
    "Let's create some data files and see how we can read them in Beam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/sample1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/sample1.txt\n",
    "This is just a plain text file, UTF-8 strings are allowed ðŸŽ‰.\n",
    "Each line in the file is one element in the PCollection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/sample2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/sample2.txt\n",
    "There are no guarantees on the order of the elements.\n",
    "à¸…^â€¢ï»Œâ€¢^à¸…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/penguins.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/penguins.csv\n",
    "species,culmen_length_mm,culmen_depth_mm,flipper_length_mm,body_mass_g\n",
    "0,0.2545454545454545,0.6666666666666666,0.15254237288135594,0.2916666666666667\n",
    "0,0.26909090909090905,0.5119047619047618,0.23728813559322035,0.3055555555555556\n",
    "1,0.5236363636363636,0.5714285714285713,0.3389830508474576,0.2222222222222222\n",
    "1,0.6509090909090909,0.7619047619047619,0.4067796610169492,0.3333333333333333\n",
    "2,0.509090909090909,0.011904761904761862,0.6610169491525424,0.5\n",
    "2,0.6509090909090909,0.38095238095238104,0.9830508474576272,0.8333333333333334"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from text files\n",
    "\n",
    "We can use theÂ [`ReadFromText`](https://beam.apache.org/releases/pydoc/current/apache_beam.io.textio.html#apache_beam.io.textio.ReadFromText)Â transform to read text files intoÂ `str`Â elements.\n",
    "\n",
    "It takes aÂ [*glob pattern*](https://en.wikipedia.org/wiki/Glob_%28programming%29)Â as an input, and reads all the files that match that pattern. It returns one element for each line in the file.\n",
    "\n",
    "For example, in the patternÂ `data/*.txt`, theÂ `*`Â is a wildcard that matches anything. This pattern matches all the files in theÂ `data/`Â directory with aÂ `.txt`Â extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is just a plain text file, UTF-8 strings are allowed ðŸŽ‰.\n",
      "Each line in the file is one element in the PCollection.\n",
      "There are no guarantees on the order of the elements.\n",
      "à¸…^â€¢ï»Œâ€¢^à¸…\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "input_files = 'data/sample*.txt'\n",
    "with beam.Pipeline() as pipeline:\n",
    "  (\n",
    "      pipeline\n",
    "      | 'Read files' >> beam.io.ReadFromText(input_files)\n",
    "      | 'Print contents' >> beam.Map(print)\n",
    "  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to text files\n",
    "\n",
    "We can use theÂ [`WriteToText`](https://beam.apache.org/releases/pydoc/2.27.0/apache_beam.io.textio.html#apache_beam.io.textio.WriteToText)Â transform to writeÂ `str`Â elements into text files.\n",
    "\n",
    "It takes aÂ *file path prefix*Â as an input, and it writes the allÂ `str`Â elements into one or more files with filenames starting with that prefix. You can optionally pass aÂ `file_name_suffix`Â as well, usually used for the file extension. Each element goes into its own line in the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "output_file_name_prefix = 'output/sample'\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "  (\n",
    "      pipeline\n",
    "      | 'Create file lines' >> beam.Create([\n",
    "          'Each element must be a string.',\n",
    "          'It writes one element per line.',\n",
    "          'There are no guarantees on the line order.',\n",
    "          'The data might be written into multiple files.',\n",
    "      ])\n",
    "      | 'Write to files' >> beam.io.WriteToText(\n",
    "          output_file_name_prefix,\n",
    "          file_name_suffix='.txt')\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each element must be a string.\n",
      "It writes one element per line.\n",
      "There are no guarantees on the line order.\n",
      "The data might be written into multiple files.\n"
     ]
    }
   ],
   "source": [
    "# Lets look at the output files and contents.\n",
    "!head output/sample*.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from a SQLite database\n",
    "\n",
    "Lets begin by creating a small SQLite local database file.\n",
    "\n",
    "Run the \"Creating the SQLite database\" cell to create a new SQLite3 database with the filename you choose. You can double-click it to see the source code if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'ðŸŒ•', '2017-12-03 15:47:00', 'Full Moon')\n",
      "(2, 'ðŸŒ—', '2017-12-10 07:51:00', 'Last Quarter')\n",
      "(3, 'ðŸŒ‘', '2017-12-18 06:30:00', 'New Moon')\n",
      "(4, 'ðŸŒ“', '2017-12-26 09:20:00', 'First Quarter')\n",
      "(5, 'ðŸŒ•', '2018-01-02 02:24:00', 'Full Moon')\n",
      "(6, 'ðŸŒ—', '2018-01-08 22:25:00', 'Last Quarter')\n",
      "(7, 'ðŸŒ‘', '2018-01-17 02:17:00', 'New Moon')\n",
      "(8, 'ðŸŒ“', '2018-01-24 22:20:00', 'First Quarter')\n",
      "(9, 'ðŸŒ•', '2018-01-31 13:27:00', 'Full Moon')\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "database_file = \"data/moon-phases.db\" #@param {type:\"string\"}\n",
    "\n",
    "with sqlite3.connect(database_file) as db:\n",
    "  cursor = db.cursor()\n",
    "\n",
    "  # Create the moon_phases table.\n",
    "  cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS moon_phases (\n",
    "      id INTEGER PRIMARY KEY,\n",
    "      phase_emoji TEXT NOT NULL,\n",
    "      peak_datetime DATETIME NOT NULL,\n",
    "      phase TEXT NOT NULL)''')\n",
    "\n",
    "  # Truncate the table if it's already populated.\n",
    "  cursor.execute('DELETE FROM moon_phases')\n",
    "\n",
    "  # Insert some sample data.\n",
    "  insert_moon_phase = 'INSERT INTO moon_phases(phase_emoji, peak_datetime, phase) VALUES(?, ?, ?)'\n",
    "  cursor.execute(insert_moon_phase, ('ðŸŒ•', '2017-12-03 15:47:00', 'Full Moon'))\n",
    "  cursor.execute(insert_moon_phase, ('ðŸŒ—', '2017-12-10 07:51:00', 'Last Quarter'))\n",
    "  cursor.execute(insert_moon_phase, ('ðŸŒ‘', '2017-12-18 06:30:00', 'New Moon'))\n",
    "  cursor.execute(insert_moon_phase, ('ðŸŒ“', '2017-12-26 09:20:00', 'First Quarter'))\n",
    "  cursor.execute(insert_moon_phase, ('ðŸŒ•', '2018-01-02 02:24:00', 'Full Moon'))\n",
    "  cursor.execute(insert_moon_phase, ('ðŸŒ—', '2018-01-08 22:25:00', 'Last Quarter'))\n",
    "  cursor.execute(insert_moon_phase, ('ðŸŒ‘', '2018-01-17 02:17:00', 'New Moon'))\n",
    "  cursor.execute(insert_moon_phase, ('ðŸŒ“', '2018-01-24 22:20:00', 'First Quarter'))\n",
    "  cursor.execute(insert_moon_phase, ('ðŸŒ•', '2018-01-31 13:27:00', 'Full Moon'))\n",
    "\n",
    "  # Query for the data in the table to make sure it's populated.\n",
    "  cursor.execute('SELECT * FROM moon_phases')\n",
    "  for row in cursor.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use a FlatMap transform to receive a SQL query and yield each result row, but that would mean creating a new database connection for each query. If we generated a large number of queries, creating that many connections could be a bottleneck.\n",
    "\n",
    "It would be nice to create the database connection only once for each worker, and every query could use the same connection if needed.\n",
    "\n",
    "We can use a custom DoFn transform for this. It allows us to open and close resources, like the database connection, only once per DoFn instance by using the setup and teardown methods.\n",
    "\n",
    "â„¹ï¸ It should be safe to read from a database with multiple concurrent processes using the same connection, but only one process should be writing at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'phase_emoji': 'ðŸŒ•', 'peak_datetime': '2017-12-03 15:47:00', 'phase': 'Full Moon'}\n",
      "{'phase_emoji': 'ðŸŒ—', 'peak_datetime': '2017-12-10 07:51:00', 'phase': 'Last Quarter'}\n",
      "{'phase_emoji': 'ðŸŒ‘', 'peak_datetime': '2017-12-18 06:30:00', 'phase': 'New Moon'}\n",
      "{'phase_emoji': 'ðŸŒ“', 'peak_datetime': '2017-12-26 09:20:00', 'phase': 'First Quarter'}\n",
      "{'phase_emoji': 'ðŸŒ•', 'peak_datetime': '2018-01-02 02:24:00', 'phase': 'Full Moon'}\n",
      "{'phase_emoji': 'ðŸŒ—', 'peak_datetime': '2018-01-08 22:25:00', 'phase': 'Last Quarter'}\n",
      "{'phase_emoji': 'ðŸŒ‘', 'peak_datetime': '2018-01-17 02:17:00', 'phase': 'New Moon'}\n",
      "{'phase_emoji': 'ðŸŒ“', 'peak_datetime': '2018-01-24 22:20:00', 'phase': 'First Quarter'}\n",
      "{'phase_emoji': 'ðŸŒ•', 'peak_datetime': '2018-01-31 13:27:00', 'phase': 'Full Moon'}\n",
      "{'phase_emoji': 'ðŸŒ•', 'phase': 'Full Moon'}\n",
      "{'phase_emoji': 'ðŸŒ—', 'phase': 'Last Quarter'}\n",
      "{'phase_emoji': 'ðŸŒ‘', 'phase': 'New Moon'}\n",
      "{'phase_emoji': 'ðŸŒ“', 'phase': 'First Quarter'}\n",
      "{'phase_emoji': 'ðŸŒ•', 'phase': 'Full Moon'}\n",
      "{'phase_emoji': 'ðŸŒ—', 'phase': 'Last Quarter'}\n",
      "{'phase_emoji': 'ðŸŒ‘', 'phase': 'New Moon'}\n",
      "{'phase_emoji': 'ðŸŒ“', 'phase': 'First Quarter'}\n",
      "{'phase_emoji': 'ðŸŒ•', 'phase': 'Full Moon'}\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import sqlite3\n",
    "from typing import Iterable, List, Tuple, Dict\n",
    "\n",
    "class SQLiteSelect(beam.DoFn):\n",
    "  def __init__(self, database_file: str):\n",
    "    self.database_file = database_file\n",
    "    self.connection = None\n",
    "\n",
    "  def setup(self):\n",
    "    self.connection = sqlite3.connect(self.database_file)\n",
    "\n",
    "  def process(self, query: Tuple[str, List[str]]) -> Iterable[Dict[str, str]]:\n",
    "    table, columns = query\n",
    "    cursor = self.connection.cursor()\n",
    "    cursor.execute(f\"SELECT {','.join(columns)} FROM {table}\")\n",
    "    for row in cursor.fetchall():\n",
    "      yield dict(zip(columns, row))\n",
    "\n",
    "  def teardown(self):\n",
    "    self.connection.close()\n",
    "\n",
    "@beam.ptransform_fn\n",
    "@beam.typehints.with_input_types(beam.pvalue.PBegin)\n",
    "@beam.typehints.with_output_types(Dict[str, str])\n",
    "def SelectFromSQLite(\n",
    "    pbegin: beam.pvalue.PBegin,\n",
    "    database_file: str,\n",
    "    queries: List[Tuple[str, List[str]]],\n",
    ") -> beam.PCollection[Dict[str, str]]:\n",
    "  return (\n",
    "      pbegin\n",
    "      | 'Create None' >> beam.Create(queries)\n",
    "      | 'SQLite SELECT' >> beam.ParDo(SQLiteSelect(database_file))\n",
    "  )\n",
    "\n",
    "queries = [\n",
    "    # (table_name, [column1, column2, ...])\n",
    "    ('moon_phases', ['phase_emoji', 'peak_datetime', 'phase']),\n",
    "    ('moon_phases', ['phase_emoji', 'phase']),\n",
    "]\n",
    "\n",
    "options = PipelineOptions(flags=[], type_check_additional='all')\n",
    "with beam.Pipeline(options=options) as pipeline:\n",
    "  (\n",
    "      pipeline\n",
    "      | 'Read from SQLite' >> SelectFromSQLite(database_file, queries)\n",
    "      | 'Print rows' >> beam.Map(print)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "343191058819caea96d5cde1bd3b1a75b4807623ce2cda0e1c8499e39ac847e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
